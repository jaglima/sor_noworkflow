{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Summer of Reproducibility - noWorkflow base experiment\n",
    "\n",
    "This Jupyter Notebook dedicated to walk you through noWorkflow applications in Data Science and Machine Learning. It is result of our work in the Summer of Reproducibility from OSPO UCSC 2023.\n",
    "\n",
    "This Notebook is an usecase based on the problem of Fraud Detection. We partially replicates the work \"The effect of feature extraction and data sampling on credit card fraud detection.\".The interested reader is invited to consult the original work [here].(https://link.springer.com/article/10.1186/s40537-023-00684-w). \n",
    "\n",
    "In this notebook we aim to assess how provenance recollection with [noWorkflow](https://github.com/gems-uff/noworkflow) works in a classical DS/ML subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_tag('dataset_reading')\n",
    "df = pd.read_csv('dataset/creditcard.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering stage\n",
    "\n",
    "Separate the features and target variable. First step in feature treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering: Apply PCA for feature extraction.\n",
    "\n",
    "Here we define hyperparam_def tag given that n_components argument in PCA is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation(id=48, checkpoint=1399.998662652, code_component_id=919, activation_id=45, repr=3)\n"
     ]
    }
   ],
   "source": [
    "pca_components = now_variable('pca_components', 3)\n",
    "pca = PCA(n_components=pca_components)  # Adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering: Apply random undersampling over the extracted features\n",
    "\n",
    "Another case of feature engineering operation with hyperparameter definition. Here is random_state value for RandmUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation(id=66, checkpoint=1404.694020646, code_component_id=952, activation_id=63, repr=42)\n"
     ]
    }
   ],
   "source": [
    "random_seed = now_variable('random_seed', 42)\n",
    "rus = RandomUnderSampler(random_state=random_seed)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_pca, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Feature engineering: Spliting dataset into train and test\n",
    "\n",
    "Here we have two hyperparameters assignments: the proportion of the test_size and the random_state. A guess here would be implement some logic to take all scalar values in hyperparam_def in cells. Not sure at the moment if there are any corner case where a hyperparameter could be vectorial or an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation(id=88, checkpoint=1407.9494737789998, code_component_id=994, activation_id=82, repr=0.2)\n"
     ]
    }
   ],
   "source": [
    "now_tag('feature_eng')\n",
    "test_dim = now_variable('test_dim', 0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=test_dim, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring: model training and transforming features into predictions\n",
    "##### RandomForest\n",
    "\n",
    "Train and evaluate Random Forest Classifier. Unsure now if adding a model_training tag would be redundant here. Scoring is enough at first sight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now_tag('scoring')\n",
    "now_tag('model_training')\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluating: evaluating the performance of models\n",
    "##### RandomForest\n",
    "Computing performance metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - ROC = 0.903370, F1 = 0.899471\n"
     ]
    }
   ],
   "source": [
    "now_tag('evaluating')\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "roc_rf = now_variable('roc_rf', roc_auc_score(y_test, y_pred_rf))\n",
    "#roc_rf = roc_auc_score(y_test, y_pred_rf)\n",
    "f1_rf = now_variable('f1_rf', f1_score(y_test, y_pred_rf))\n",
    "#f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest - ROC = %f, F1 = %f\" % (roc_rf, f1_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment comparision\n",
    "\n",
    "Save the operations dictionary in a shelve object with this trial_id as a key.\n",
    "\n",
    "Steps are:\n",
    "1. calls get_pre for a given tagged variable and keeps the operations_dictionary output\n",
    "2. calls store operations() to store the dict into a shelve object with this trial_id key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary stored in shelve.\n"
     ]
    }
   ],
   "source": [
    "ops_dict = get_pre('roc_rf')\n",
    "\n",
    "trial_id = __noworkflow__.trial_id\n",
    "store_operations(trial_id, ops_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noWorkflow 3",
   "language": "python",
   "name": "noworkflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd46906d0be51105938edee03e9704979453c4958d5b4d09c310e6ecda521c36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
